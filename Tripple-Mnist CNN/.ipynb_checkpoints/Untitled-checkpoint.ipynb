{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e2e8932b-d91e-4f56-b972-9ec0e5b7329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1ee70e3e-1aaa-41d4-9130-138497ef5684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmultioutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform_average'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Mean absolute error regression loss.\n",
       "\n",
       "Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
       "    Ground truth (correct) target values.\n",
       "\n",
       "y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
       "    Estimated target values.\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
       "    Defines aggregating of multiple output values.\n",
       "    Array-like value defines weights used to average errors.\n",
       "\n",
       "    'raw_values' :\n",
       "        Returns a full set of errors in case of multioutput input.\n",
       "\n",
       "    'uniform_average' :\n",
       "        Errors of all outputs are averaged with uniform weight.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "loss : float or ndarray of floats\n",
       "    If multioutput is 'raw_values', then mean absolute error is returned\n",
       "    for each output separately.\n",
       "    If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
       "    weighted average of all output errors is returned.\n",
       "\n",
       "    MAE output is non-negative floating point. The best value is 0.0.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import mean_absolute_error\n",
       ">>> y_true = [3, -0.5, 2, 7]\n",
       ">>> y_pred = [2.5, 0.0, 2, 8]\n",
       ">>> mean_absolute_error(y_true, y_pred)\n",
       "0.5\n",
       ">>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
       ">>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
       ">>> mean_absolute_error(y_true, y_pred)\n",
       "0.75\n",
       ">>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
       "array([0.5, 1. ])\n",
       ">>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
       "0.85...\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\salij\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages\\sklearn\\metrics\\_regression.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sklearn.metrics.mean_absolute_error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3f0d65b4-7eee-4a39-9d60-48eb0f909fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path \n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score,learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd01296a-f204-44ba-91f0-a0b16cc9f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for fname in files:\n",
    "            if Path(fname).suffix == '.png':\n",
    "                image_path = os.path.join(root, fname)\n",
    "                image = cv2.imread(image_path)\n",
    "                label = os.path.basename(root)\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0404b8d1-4e80-450c-9f7d-85cb1ad1abdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 64000 images loaded with 640 categories\n",
      "Validation: 16000 images loaded with 160 categories\n",
      "Test: 20000 images loaded with 200 categories\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'triple_mnist/'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "# Load training images and labels\n",
    "train_images, train_labels = load_images_and_labels(train_dir)\n",
    "num_train_images = len(train_images)\n",
    "num_train_categories = len(set(train_labels))\n",
    "\n",
    "print(f\"Training: {num_train_images} images loaded with {num_train_categories} categories\")\n",
    "\n",
    "# Load validation images and labels\n",
    "val_images, val_labels = load_images_and_labels(val_dir)\n",
    "num_val_images = len(val_images)\n",
    "num_val_categories = len(set(val_labels))\n",
    "\n",
    "print(f\"Validation: {num_val_images} images loaded with {num_val_categories} categories\")\n",
    "\n",
    "# Load test images and labels\n",
    "test_images, test_labels = load_images_and_labels(test_dir)\n",
    "num_test_images = len(test_images)\n",
    "num_test_categories = len(set(test_labels))\n",
    "\n",
    "print(f\"Test: {num_test_images} images loaded with {num_test_categories} categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a852d7-3b59-457a-ac6e-cd9a9cf00d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select 5 random indices from the training dataset\n",
    "random_indices = random.sample(range(num_train_images), 6)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10, 7))\n",
    "\n",
    "# Reshape the axes array to a flat 1D array\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Visualize the random samples\n",
    "for i, index in enumerate(random_indices):\n",
    "    image = train_images[index]\n",
    "    label = train_labels[index]\n",
    "\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(f\"Label: {label}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(len(random_indices), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1080b389-10a9-44f0-89c2-7c6bfb71e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_array = np.array(train_images) \n",
    "train_labels_array = np.array(train_labels)\n",
    "\n",
    "val_images_array = np.array(val_images) \n",
    "val_labels_array= np.array(val_labels)\n",
    "\n",
    "test_images_array = np.array(test_images) \n",
    "test_labels_array= np.array(test_labels)\n",
    "\n",
    "\n",
    "\n",
    "X = np.concatenate((train_images_array, val_images_array), axis=0)\n",
    "y = np.concatenate((train_labels_array, val_labels_array), axis=0)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, y_test = test_images_array, test_labels_array"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91af66c0-5c11-4879-8559-9cc1f0b10aea",
   "metadata": {},
   "source": [
    "# Encode target values\n",
    "from keras.utils import to_categorical\n",
    "y_train_enc = to_categorical(y_train)\n",
    "y_test_enc = to_categorical(y_test)\n",
    "y_val_enc = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d73140b-1f42-45b3-8ae0-28b935c2a204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (64000, 7056)\n",
      "y_train shape: (64000,)\n",
      "x_test shape: (20000, 7056)\n",
      "y_test shape:  (20000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16000, 7056)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaling training images\n",
    "X_train_gray = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in X_train])\n",
    "X_test_gray = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in X_test])\n",
    "X_val_gray = np.array([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in X_val])\n",
    "\n",
    "\n",
    "# Flatten the grayscale images\n",
    "X_train_flat = X_train_gray.reshape(X_train_gray.shape[0], -1)\n",
    "X_test_flat = X_test_gray.reshape(X_test_gray.shape[0], -1)\n",
    "X_val_flat = X_val_gray.reshape(X_val_gray.shape[0], -1)\n",
    "\n",
    "\n",
    "print('x_train shape: ', X_train_flat.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "#print(y_train_flat.shape)\n",
    "print('x_test shape:', X_test_flat.shape)\n",
    "#print(y_test_flat.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "X_val_flat.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a78bfe3-6236-46c4-b60a-106fc47cc013",
   "metadata": {},
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "X_test_scaled = scaler.transform(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d43099ea-844e-4bec-8ca0-e923fc8941e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(criterion = 'entropy', max_depth =5, random_state=42)\n",
    "\n",
    "#train the the decision tree on the train_x train_y train sets\n",
    "dt_clf = dt_clf.fit(X_train_flat, y_train)\n",
    "\n",
    "y_pred_val = dt_clf.predict(X_val_flat)\n",
    "\n",
    "# if len(y_test.shape) > 1:\n",
    "#     y_test_flat = y_test.reshape(y_test.shape[0], -1)\n",
    "\n",
    "score = dt_clf.score(X_test_flat, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "673779e3-6bb1-4681-ae4e-73b61778577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Define the Decision Tree classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split':[2,5,10],\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(dt_clf, param_grid, cv=2, scoring='accuracy')\n",
    "grid_search.fit(X_train_flat, y_train)\n",
    "#grid_search.fit(X, y)\n",
    "# Get the best hyperparameters\n",
    "best_dt_clf = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbac2cca-872f-4170-a5d5-62d615f3e31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Metrics:\n",
      "Accuracy: 0.0019375\n",
      "F1 Score: 0.0002570854448958861\n",
      "Confusion Matrix:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [3 0 0 ... 0 0 0]\n",
      " [2 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [2 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the validation set\n",
    "y_pred_val = best_dt_clf.predict(X_val_flat)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "f1_val = f1_score(y_val, y_pred_val, average='weighted')\n",
    "conf_matrix_val = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "print(\"Validation Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_val)\n",
    "print(\"F1 Score:\", f1_val)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c429f53-74a0-4c1f-8b21-6259f2aa77b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Metrics:\n",
      "Accuracy: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [4 0 0 ... 0 0 0]\n",
      " [9 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "y_pred_test = best_dt_clf.predict(X_test_flat)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"F1 Score:\", f1_test)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe34588-b49f-4f9c-ba06-ec3764b55514",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(np.float32)/ 255.0\n",
    "y_val = y_val.astype(np.float32) / 255.0\n",
    "y_test = y_test.astype(np.float32) /255.0\n",
    "X_test = X_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280009d6-bd2f-4a18-8f76-4acf99164998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 82, 82, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 41, 41, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 39, 39, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 19, 19, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 11552)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                739392    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              65000     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 809480 (3.09 MB)\n",
      "Trainable params: 809480 (3.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reshape the flattened data to the original shape\n",
    "input_shape = (84, 84, 3) \n",
    "\n",
    "# Build the CNN model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the output to feed into dense layers\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Add dense layers\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1000, activation='softmax'))  # Assuming 10 classes, adjust accordingly\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03d27b1-c2cd-4ea8-b375-58aab80fa40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 108s 54ms/step - loss: 0.6857 - accuracy: 0.0012 - val_loss: 0.4137 - val_accuracy: 3.7500e-04\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 109s 54ms/step - loss: 0.3300 - accuracy: 0.0014 - val_loss: 0.3479 - val_accuracy: 3.7500e-04\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 109s 54ms/step - loss: 0.2287 - accuracy: 0.0014 - val_loss: 0.3313 - val_accuracy: 3.7500e-04\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 109s 54ms/step - loss: 0.1647 - accuracy: 0.0014 - val_loss: 0.3616 - val_accuracy: 3.7500e-04\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 109s 54ms/step - loss: 0.1257 - accuracy: 0.0014 - val_loss: 0.3542 - val_accuracy: 3.7500e-04\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 108s 54ms/step - loss: 0.1039 - accuracy: 0.0015 - val_loss: 0.4140 - val_accuracy: 3.7500e-04\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 108s 54ms/step - loss: 0.0858 - accuracy: 0.0014 - val_loss: 0.4299 - val_accuracy: 3.7500e-04\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 109s 54ms/step - loss: 0.0729 - accuracy: 0.0014 - val_loss: 0.4336 - val_accuracy: 3.7500e-04\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 109s 55ms/step - loss: 0.0703 - accuracy: 0.0015 - val_loss: 0.5366 - val_accuracy: 3.7500e-04\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 108s 54ms/step - loss: 0.0659 - accuracy: 0.0015 - val_loss: 0.4870 - val_accuracy: 3.7500e-04\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.4870 - accuracy: 3.7500e-04\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 6.3972 - accuracy: 0.0049\n",
      "validation Accuracy: 0.000375000003259629\n",
      "validation loss: 0.487033873796463\n",
      "Test Accuracy: 0.0048500001430511475\n",
      "Test loss: 6.397193431854248\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"validation Accuracy:\", val_accuracy)\n",
    "print(\"validation loss:\", val_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc55e584-4eb3-4f27-85c2-22b7fb57246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 5s 10ms/step\n",
      "\n",
      "F1 Score on validation data: 0.8965728729051775\n"
     ]
    }
   ],
   "source": [
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_classes_val = np.argmax(y_pred_val, axis=1)\n",
    "\n",
    "y_val = y_val.astype(int)\n",
    "y_pred_classes_val = y_pred_classes_val.astype(int)\n",
    "# Compute the F1 score\n",
    "f1_val = f1_score(y_val, y_pred_classes_val, average='weighted')\n",
    "print(\"\\nF1 Score on validation data:\", f1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcf6ce78-f2ba-4837-8196-975bf41298ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 10s 16ms/step\n",
      "\n",
      "F1 Score on test data: 0.11376834790037695\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_test = y_test.astype(int)\n",
    "y_pred_classes = y_pred_classes.astype(int)\n",
    "# Compute the F1 score\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print(\"\\nF1 Score on test data:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72f51e48-1805-4ad1-adf9-9ee007646ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 3\n",
    "num_parts = 3\n",
    "\n",
    "def split_image(image):\n",
    "    height, width, channels = image.shape\n",
    "    part_width = width // num_parts\n",
    "    \n",
    "    image_parts = [image[:, i * part_width: (i + 1) * part_width, :] for i in range(num_parts)]\n",
    "    \n",
    "    return image_parts\n",
    "\n",
    "# Apply the split_image function to each image in the dataset\n",
    "X_train_parts = np.array([split_image(image) for image in X_train])\n",
    "X_test_parts = np.array([split_image(image) for image in X_test])\n",
    "X_val_parts = np.array([split_image(image) for image in X_val])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f09f6869-4752-474b-9a00-ccff4da898e9",
   "metadata": {},
   "source": [
    "\n",
    "def split_labels(labels):\n",
    "    #label_parts = [label] * num_parts \n",
    "    height, width, channels = labels.shape\n",
    "    part_size = width // num_parts\n",
    "    \n",
    "    label_parts = [labels[:, i * part_size: (i + 1) * part_size, :] for i in range(num_parts)]\n",
    "    \n",
    "    return label_parts\n",
    "\n",
    "\n",
    "y_train_parts = np.array([split_labels(labels) for labels in y_train])\n",
    "#y_test_parts = np.array([split_labels(labels) for labels in y_test])\n",
    "y_val_parts = np.array([split_labels(labels) for labels in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a3c1eb59-f8ba-44c2-bc2d-3efcfd85a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_label(label):\n",
    "    label_parts = [label] * num_parts  # Assuming label is one-dimensional\n",
    "    return label_parts\n",
    "\n",
    "# Apply the split_label function to each label in the dataset\n",
    "y_train_parts = np.array([split_label(label) for label in y_train])\n",
    "#y_test_parts = np.array([split_labels(labels) for labels in y_test])\n",
    "y_val_parts = np.array([split_label(label) for label in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9faae465-75b2-4884-b1fd-5956c89d6477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_parts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e964eb8-56b1-49e6-b8a7-2a1936b46977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_parts.shape[4]\n",
    "#position 0 = datapoints 64000\n",
    "#position 1 = channels 3\n",
    "#position 2 = height 84\n",
    "#position 3 = width 28\n",
    "#position 4 = channels 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5700659a-c30c-44ff-91f5-e81ce4d9d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 192000\n",
      "Number of training labels: 64000\n",
      "Number of validation samples: 48000\n",
      "Number of validation labels: 16000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples:\", X_train_parts_cnn.shape[0])\n",
    "print(\"Number of training labels:\", y_train.shape[0])\n",
    "\n",
    "print(\"Number of validation samples:\", X_val_parts_cnn.shape[0])\n",
    "print(\"Number of validation labels:\", y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5a605300-f2b8-4516-8e1a-7d01de30ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define height and part_width\n",
    "height = X_train_parts.shape[2]\n",
    "part_width = X_train_parts.shape[3]\n",
    "channels = X_train_parts.shape[4]\n",
    "\n",
    "# Reshape the X data for the CNN\n",
    "X_train_parts_cnn = X_train_parts.reshape(X_train_parts.shape[0] * num_parts, height, part_width, channels)\n",
    "X_test_parts_cnn = X_test_parts.reshape(X_test_parts.shape[0] * num_parts, height, part_width, channels)\n",
    "X_val_parts_cnn = X_val_parts.reshape(X_val_parts.shape[0] * num_parts, height, part_width, channels)\n",
    "\n",
    "# Reshape the label data for the model\n",
    "y_train_parts_flat = y_train_parts.reshape(y_train_parts.shape[0] * num_parts)\n",
    "y_val_parts_flat = y_val_parts.reshape(y_val_parts.shape[0] * num_parts)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4252d713-2658-4a0a-a9c7-4991436435e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df837ec4-d5c6-4273-8b41-32fae379755c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 858/6000 [===>..........................] - ETA: 24:21 - loss: 1.8814 - accuracy: 3.6422e-04"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', input_shape=(height, part_width, channels)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Dropout(0.25))\n",
    "\n",
    "# model.add(layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "# model.add(layers.BatchNormalization())\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))  # Adjust num_classes based on your task\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_parts_cnn, y_train_parts_flat, epochs=10, validation_data=(X_test_parts_cnn, y_test))\n",
    "\n",
    "y_pred_part1 = model.predict(X_val_parts_cnn[:, :, :, 0:1])  # Use 0:1 to keep the last dimension\n",
    "y_pred_part2 = model.predict(X_val_parts_cnn[:, :, :, 1:2])\n",
    "y_pred_part3 = model.predict(X_val_parts_cnn[:, :, :, 2:3])\n",
    "\n",
    "y_pred = np.column_stack([y_pred_part1, y_pred_part2, y_pred_part3])\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a51a3-f9f7-492d-a2b4-3f4422cf35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =  y_test\n",
    "y_pred = np.column_stack([y_pred_part1, y_pred_part2, y_pred_part3])\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f'Weighted F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eb3fabe7-d00a-4893-b836-34693437c00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 823/3000 [=======>......................] - ETA: 5:35 - loss: 2.3641 - accuracy: 6.2652e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train model on each part's data\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_parts_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_parts_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_parts_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_parts_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Make predictions on each part\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# y_pred_part1 = model.predict(X_test_parts_cnn[:, :, :, 0]) \u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# y_pred_part2 = model.predict(X_test_parts_cnn[:, :, :, 1])\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# y_pred_part3 = model.predict(X_test_parts_cnn[:, :, :, 2])\u001b[39;00m\n\u001b[0;32m     23\u001b[0m y_pred_part1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_parts_cnn[:, :, :, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Use 0:1 to keep the last dimension\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(height, part_width, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model on each part's data\n",
    "model.fit(X_train_parts_cnn, y_train_parts_flat, epochs=5, batch_size=64, \n",
    "          validation_data=(X_val_parts_cnn, y_val_parts_flat))\n",
    "          \n",
    "# Make predictions on each part\n",
    "# y_pred_part1 = model.predict(X_test_parts_cnn[:, :, :, 0]) \n",
    "# y_pred_part2 = model.predict(X_test_parts_cnn[:, :, :, 1])\n",
    "# y_pred_part3 = model.predict(X_test_parts_cnn[:, :, :, 2])\n",
    "\n",
    "y_pred_part1 = model.predict(X_test_parts_cnn[:, :, :, 0:1])  # Use 0:1 to keep the last dimension\n",
    "y_pred_part2 = model.predict(X_test_parts_cnn[:, :, :, 1:2])\n",
    "y_pred_part3 = model.predict(X_test_parts_cnn[:, :, :, 2:3])\n",
    "\n",
    "\n",
    "# Concatenate predictions into final 3-digit prediction\n",
    "y_pred = np.column_stack([y_pred_part1, y_pred_part2, y_pred_part3])\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
